% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/logRegCV.R
\name{logRegCV}
\alias{logRegCV}
\title{Logistic regression nested k-fold cross-validation}
\usage{
logRegCV(df, colname.response, stratify = T, k.outer = 10,
  standardize = T, na.replace = "median", sd.zero.replace = T,
  balance = F, k.inner = 10, alpha = 1, type.measure = "class",
  nlambda = 100, lower.limits = 0, predictLambda = "lambda.min", ...)
}
\arguments{
\item{df}{A dataframe of observations as rows and features as columns. \strong{Important:} input matrix/dataframe should
include the response vector as a column.}

\item{colname.response}{The name of a column containing the response classes.}

\item{stratify}{Stratification ensures that all classes are present in every train/test set when splitting the
data into k-folds (k.outer).}

\item{k.outer}{Number of folds to split the data in the outer CV loop.}

\item{standardize}{Transform each feature column by (x - mean(x))/sd(x). Standardization is done before class
balancing. Standardization scales each feature so that the values are comparable across features. In stochastic gradient
descent, feature scaling can improve the convergence speed of the algorithm.}

\item{na.replace}{During feature standardization, imputation of NA feature values by 'mean' or 'median'.}

\item{sd.zero.replace}{During feature standardization, if standard deviation of a feature is 0, standardization will fail
due to divide-by-zero error. If sd.zero.replace = TRUE, a vector of zeros will be returned instead.}

\item{balance}{Balancing of classes by up/down sampling.}

\item{k.inner}{Number of folds to split the data in the inner CV loop.}

\item{alpha}{From \pkg{glmnet::cv.glmnet()}. Elastic net regularization parameter. 0: ridge, 1: lasso.}

\item{type.measure}{From \pkg{glmnet::cv.glmnet()}. Type of measure to determine optimal lambda value.}

\item{nlambda}{From \pkg{glmnet::cv.glmnet()}. Number of lambda values to try to determine optimal lambda value.}

\item{lower.limits}{From \pkg{glmnet::cv.glmnet()}. . Lower limit of
coefficients. Default: 0 forces coefficients to be non-negative.}

\item{predictLambda}{{From \pkg{glmnet::cv.glmnet()}. For prediction on test set use the lambda value with the lowest
error ('lambda.min') or the lambda value one standard error higher than lambda.min ('lambda.1se')}.}

\item{...}{Other arguments that can be passed to \pkg{glmnet::cv.glmnet()}.}
}
\value{
Returns a list of length(k.outer), with sublists containing (1) a cv.glmnet object from the training on the outer
fold train set, and (2) a dataframe of probabilities from the prediction on the outer fold test set.
}
\description{
Performs nested k-fold cross-validation to assess the performance of a logistic regression model.
\strong{Important:} input matrix/dataframe should include the response vector as a column. Inner folds: uses
\pkg{glmnet::cv.glmnet()} fit a logistic regression to the outer fold training set (i.e. determine the optimal lambda and
subsequently the regression coefficients). Outer folds: uses the model created from the inner fold to predict on the outer
fold test set.
}
