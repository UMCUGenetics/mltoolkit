% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/calcPerf.R
\name{calcPerfMC}
\alias{calcPerfMC}
\title{Calculate classifier performance metrics (multi-class)}
\usage{
calcPerfMC(confusion, metrics, avg.method = NULL, melt = F)
}
\arguments{
\item{confusion}{A matrix, data frame; or a list of matrices or data frames. These should contain a confusion matrix for
one cutoff or confusion matrices for multiple cutoffs.}

\item{metrics}{A character vector of the desired statistics. Multiple metrics can be specified; however, usually only two are used for
binary classification statistics plots.}

\item{avg.method}{'macro': Simple average. Performance metrics are calculated individually for each class, and then averaged.
'weighted': Weighted average. Similar to 'macro', except that the performance metrics for each class are weighted by the 
relative contribution of each class (i.e. classes with more samples have more weighting)}

\item{melt}{If melt = FALSE, output in wide format. If melt = TRUE, output in long format}
}
\value{
A numeric vector or matrix of the selected performance metrics
}
\description{
Calculate classifier performance metrics (multi-class)
}
\examples{
calcPerf(confusion, c('tpr','tnr'), avg.method = 'macro')
}
