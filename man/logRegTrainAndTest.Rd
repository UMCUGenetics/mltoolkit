% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/logRegTrainAndTest.R
\name{logRegTrainAndTest}
\alias{logRegTrainAndTest}
\title{Logistic regression train and test workflow}
\usage{
logRegTrainAndTest(train, test = NULL, colname.response,
  positive.response = NULL, pre.transform.formula = NULL, standardize = F,
  na.replace = "median", sd.zero.bypass = T, balance = F, k = 10,
  alpha = 1, type.measure = "class", nlambda = 100, lower.limits = 0,
  predictLambda = "lambda.min", ...)
}
\arguments{
\item{train}{Train dataset. A dataframe of observations as rows and features as columns. \strong{Important:} input
matrix/dataframe should include the response vector as a column.}

\item{test}{Test dataset. A dataframe of observations as rows and features as columns. \strong{Important:} input
matrix/dataframe should include the response vector as a column.}

\item{colname.response}{The name of a column containing the response classes.}

\item{positive.response}{The name of the positive class (i.e. the class one wants to predict).}

\item{standardize}{Transform each feature column by (x - mean(x))/sd(x). Standardization is done before class
balancing. Standardization scales each feature so that the values are comparable across features. In stochastic gradient
descent, feature scaling can improve the convergence speed of the algorithm.}

\item{na.replace}{During feature standardization, imputation of NA feature values by 'mean' or 'median'.}

\item{sd.zero.bypass}{During feature standardization, if standard deviation of a feature is 0, standardization will fail
due to divide-by-zero error. If sd.zero.bypass = TRUE, a vector of zeros will be returned instead.}

\item{balance}{Balancing of classes by up/down sampling.}

\item{k}{Number of folds to split the data in when fitting the regression with glmnet::cv.glmnet().}

\item{alpha}{From \pkg{glmnet::cv.glmnet()}. Elastic net regularization parameter. 0: ridge, 1: lasso.}

\item{type.measure}{From \pkg{glmnet::cv.glmnet()}. Type of measure to determine optimal lambda value.}

\item{nlambda}{From \pkg{glmnet::cv.glmnet()}. Number of lambda values to try to determine optimal lambda value.}

\item{lower.limits}{From \pkg{glmnet::cv.glmnet()}. Lower limit of coefficients. Default: 0 forces coefficients to be 
non-negative.}

\item{predictLambda}{{From \pkg{glmnet::cv.glmnet()}. For prediction on test set use the lambda value with the lowest
error ('lambda.min') or the lambda value one standard error higher than lambda.min ('lambda.1se')}.}

\item{...}{Other arguments that can be passed to \pkg{glmnet::cv.glmnet()}.}
}
\value{
Omitting a test set only returns a glmnet object, while  Including a test set returns a list containing the glmnet 
object and prediction probabilities
}
\description{
Logistic regression train and test workflow
}
\examples{
## Omitting a test set only returns a glmnet object
logRegTrainAndTest(train, colname.response = 'response', positive.response = 'BRCA', 
                   standardize = T, balance = 'up')

## Including a test set returns a list containing the glmnet object and prediction probabilities
logRegTrainAndTest(train, test, colname.response = 'response', positive.response = 'BRCA', 
                   standardize = T, balance = 'up')

}
